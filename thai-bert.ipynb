{
    "cells": [
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "imports",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Import libraries\n",
                "import pandas as pd\n",
                "import numpy as np\n",
                "import torch\n",
                "from transformers import AutoTokenizer, AutoModelForSequenceClassification, AdamW, get_linear_schedule_with_warmup\n",
                "from torch.utils.data import Dataset, DataLoader\n",
                "from sklearn.model_selection import train_test_split\n",
                "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, precision_score, recall_score, f1_score\n",
                "import matplotlib.pyplot as plt\n",
                "import seaborn as sns\n",
                "from tqdm import tqdm\n",
                "import re\n",
                "import emoji\n",
                "from pythainlp.tokenize import word_tokenize\n",
                "import warnings\n",
                "warnings.filterwarnings('ignore')"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "preprocessing",
            "metadata": {},
            "source": [
                "## 1. Data Preprocessing"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "preprocessing_functions",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Preprocessing functions from model.ipynb\n",
                "def clean_text(text):\n",
                "    if pd.isna(text):\n",
                "        return text\n",
                "    \n",
                "    text = str(text)\n",
                "    \n",
                "    # Remove URLs\n",
                "    text = re.sub(r'http[s]?://(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
                "    text = re.sub(r'www\\.(?:[a-zA-Z]|[0-9]|[$-_@.&+]|[!*\\\\(\\\\),]|(?:%[0-9a-fA-F][0-9a-fA-F]))+', '', text)\n",
                "    \n",
                "    # Normalize emojis (convert to text description)\n",
                "    text = emoji.demojize(text, language='en')\n",
                "    \n",
                "    return text.strip()\n",
                "\n",
                "def lowercase_english(text):\n",
                "    if pd.isna(text):\n",
                "        return text\n",
                "    \n",
                "    result = []\n",
                "    for char in str(text):\n",
                "        # Check if character is English\n",
                "        if ord(char) < 128:\n",
                "            result.append(char.lower())\n",
                "        else:\n",
                "            result.append(char)\n",
                "    \n",
                "    return ''.join(result)\n",
                "\n",
                "def calculate_thai_length(text):\n",
                "    if pd.isna(text):\n",
                "        return 0\n",
                "    \n",
                "    # Tokenize using PyThaiNLP\n",
                "    tokens = word_tokenize(str(text), engine='newmm')\n",
                "    return len(tokens)\n",
                "\n",
                "def prepare_dataset(dataset_file):\n",
                "    print(f\"Processing {dataset_file}...\")\n",
                "    \n",
                "    df = pd.read_csv(dataset_file)\n",
                "    print(f\"  Original shape: {df.shape}\")\n",
                "    \n",
                "    # Handle missing labels (drop rows with missing category or texts)\n",
                "    df = df.dropna(subset=['category', 'texts'])\n",
                "    print(f\"  After dropping missing labels: {df.shape}\")\n",
                "    \n",
                "    # Clean text (remove URLs, normalize emojis)\n",
                "    df['texts'] = df['texts'].apply(clean_text)\n",
                "    \n",
                "    # Lowercase English characters (keep Thai)\n",
                "    df['texts'] = df['texts'].apply(lowercase_english)\n",
                "    \n",
                "    # Drop rows with empty texts after cleaning\n",
                "    df = df[df['texts'].str.strip() != '']\n",
                "    print(f\"  After removing empty texts: {df.shape}\")\n",
                "    \n",
                "    # Map labels - pos -> neu\n",
                "    df['category'] = df['category'].replace({'pos': 'neu'})\n",
                "    \n",
                "    # map to binary: neg vs non-neg\n",
                "    df['category'] = df['category'].map({'neg': 1, 'neu': 0})\n",
                "    \n",
                "    # Recalculate length using PyThaiNLP word tokenization\n",
                "    df['length'] = df['texts'].apply(calculate_thai_length)\n",
                "    \n",
                "    # Add dataset source\n",
                "    df['source'] = dataset_file.split('/')[-1]\n",
                "    \n",
                "    print(f\"  Final shape: {df.shape}\")\n",
                "    print(f\"  Label distribution: {df['category'].value_counts().to_dict()}\")\n",
                "    print()\n",
                "    \n",
                "    return df"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "load_data",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Processing datasets/dataset1.csv...\n",
                        "  Original shape: (23545, 3)\n",
                        "  After dropping missing labels: (23545, 3)\n",
                        "  After removing empty texts: (23542, 3)\n",
                        "  Final shape: (23542, 4)\n",
                        "  Label distribution: {0: 17402, 1: 6140}\n",
                        "\n",
                        "Processing datasets/dataset2.csv...\n",
                        "  Original shape: (2160, 3)\n",
                        "  After dropping missing labels: (2160, 3)\n",
                        "  After removing empty texts: (2160, 3)\n",
                        "  Final shape: (2160, 4)\n",
                        "  Label distribution: {1: 1332, 0: 828}\n",
                        "\n",
                        "Processing datasets/dataset3.csv...\n",
                        "  Original shape: (4953, 3)\n",
                        "  After dropping missing labels: (4953, 3)\n",
                        "  After removing empty texts: (4953, 3)\n",
                        "  Final shape: (4953, 4)\n",
                        "  Label distribution: {0: 4428, 1: 525}\n",
                        "\n",
                        "Combined shape before deduplication: (30655, 4)\n",
                        "Combined shape after deduplication: (30620, 4)\n",
                        "Total samples: 30620\n",
                        "\n",
                        "Label distribution:\n",
                        "category\n",
                        "0    22634\n",
                        "1     7986\n",
                        "Name: count, dtype: int64\n",
                        "\n",
                        "Length statistics:\n",
                        "count    30620.000000\n",
                        "mean        42.733867\n",
                        "std        124.197009\n",
                        "min          1.000000\n",
                        "25%          7.000000\n",
                        "50%         16.000000\n",
                        "75%         37.000000\n",
                        "max       3105.000000\n",
                        "Name: length, dtype: float64\n"
                    ]
                }
            ],
            "source": [
                "# Load and combine datasets (same as model.ipynb)\n",
                "df1 = prepare_dataset('datasets/dataset1.csv')\n",
                "df2 = prepare_dataset('datasets/dataset2.csv')\n",
                "df3 = prepare_dataset('datasets/dataset3.csv')\n",
                "\n",
                "# Combine all datasets\n",
                "all_dfs = [df1, df2, df3]\n",
                "combined_df = pd.concat(all_dfs, ignore_index=True)\n",
                "print(f\"Combined shape before deduplication: {combined_df.shape}\")\n",
                "\n",
                "# Remove duplicated samples\n",
                "combined_df = combined_df.drop_duplicates(subset=['texts'], keep='first')\n",
                "print(f\"Combined shape after deduplication: {combined_df.shape}\")\n",
                "\n",
                "# Display final statistics\n",
                "print(f\"Total samples: {len(combined_df)}\")\n",
                "print(f\"\\nLabel distribution:\")\n",
                "print(combined_df['category'].value_counts())\n",
                "print(f\"\\nLength statistics:\")\n",
                "print(combined_df['length'].describe())"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "64d3e05b",
            "metadata": {},
            "outputs": [
                {
                    "data": {
                        "text/plain": [
                            "(30620, 4)"
                        ]
                    },
                    "execution_count": 5,
                    "metadata": {},
                    "output_type": "execute_result"
                }
            ],
            "source": [
                "combined_df.shape"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 6,
            "id": "train_test_split",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Train samples: 24496\n",
                        "Test samples: 6124\n",
                        "Train distribution: {0: 18107, 1: 6389}\n",
                        "Test distribution: {0: 4527, 1: 1597}\n"
                    ]
                }
            ],
            "source": [
                "# Train/test split (same as model.ipynb)\n",
                "X = combined_df['texts'].values\n",
                "y = combined_df['category'].values\n",
                "\n",
                "X_train, X_test, y_train, y_test = train_test_split(\n",
                "    X, y, test_size=0.20, random_state=42, stratify=y\n",
                ")\n",
                "\n",
                "print(f\"Train samples: {len(X_train)}\")\n",
                "print(f\"Test samples: {len(X_test)}\")\n",
                "print(f\"Train distribution: {pd.Series(y_train).value_counts().to_dict()}\")\n",
                "print(f\"Test distribution: {pd.Series(y_test).value_counts().to_dict()}\")"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "model_setup",
            "metadata": {},
            "source": [
                "## 2. Model Setup and Training Configuration"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 7,
            "id": "config",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Device: cpu\n",
                        "Max length: 64\n",
                        "Batch size: 8\n",
                        "Epochs: 3\n",
                        "Learning rate: 2e-05\n"
                    ]
                }
            ],
            "source": [
                "# Training configuration (matching model.ipynb)\n",
                "EPOCHS = 3\n",
                "LEARNING_RATE = 2e-5\n",
                "WARMUP_STEPS = 100\n",
                "MAX_LENGTH = 64 if not torch.cuda.is_available() else 128\n",
                "BATCH_SIZE = 8 if not torch.cuda.is_available() else 16\n",
                "\n",
                "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "print(f\"Device: {device}\")\n",
                "print(f\"Max length: {MAX_LENGTH}\")\n",
                "print(f\"Batch size: {BATCH_SIZE}\")\n",
                "print(f\"Epochs: {EPOCHS}\")\n",
                "print(f\"Learning rate: {LEARNING_RATE}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "models_list",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Models to compare:\n",
                        "  - WangchanBERTa: airesearch/wangchanberta-base-att-spm-uncased\n",
                        "  - XLM-RoBERTa: xlm-roberta-base\n",
                        "  - PhayaThaiBERT: clicknext/phayathaibert\n"
                    ]
                }
            ],
            "source": [
                "# Define 3 BERT models to compare\n",
                "MODELS = {\n",
                "    'WangchanBERTa': 'airesearch/wangchanberta-base-att-spm-uncased',\n",
                "    'XLM-RoBERTa': 'xlm-roberta-base',\n",
                "    'PhayaThaiBERT': 'clicknext/phayathaibert'\n",
                "}\n",
                "\n",
                "print(\"Models to compare:\")\n",
                "for name, model_id in MODELS.items():\n",
                "    print(f\"  - {name}: {model_id}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 9,
            "id": "dataset_class",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Dataset class\n",
                "class ToxicityDataset(Dataset):\n",
                "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
                "        self.texts = texts\n",
                "        self.labels = labels\n",
                "        self.tokenizer = tokenizer\n",
                "        self.max_length = max_length\n",
                "    \n",
                "    def __len__(self):\n",
                "        return len(self.texts)\n",
                "    \n",
                "    def __getitem__(self, idx):\n",
                "        text = str(self.texts[idx])\n",
                "        label = self.labels[idx]\n",
                "        \n",
                "        encoding = self.tokenizer(\n",
                "            text,\n",
                "            add_special_tokens=True,\n",
                "            max_length=self.max_length,\n",
                "            padding='max_length',\n",
                "            truncation=True,\n",
                "            return_tensors='pt'\n",
                "        )\n",
                "        \n",
                "        return {\n",
                "            'input_ids': encoding['input_ids'].flatten(),\n",
                "            'attention_mask': encoding['attention_mask'].flatten(),\n",
                "            'labels': torch.tensor(label, dtype=torch.long)\n",
                "        }"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 10,
            "id": "train_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Training function\n",
                "def train_epoch(model, dataloader, optimizer, scheduler, device):\n",
                "    model.train()\n",
                "    total_loss = 0\n",
                "    \n",
                "    progress_bar = tqdm(dataloader, desc=\"Training\")\n",
                "    for batch in progress_bar:\n",
                "        input_ids = batch['input_ids'].to(device)\n",
                "        attention_mask = batch['attention_mask'].to(device)\n",
                "        labels = batch['labels'].to(device)\n",
                "        \n",
                "        outputs = model(\n",
                "            input_ids=input_ids,\n",
                "            attention_mask=attention_mask,\n",
                "            labels=labels\n",
                "        )\n",
                "        \n",
                "        loss = outputs.loss\n",
                "        total_loss += loss.item()\n",
                "        \n",
                "        optimizer.zero_grad()\n",
                "        loss.backward()\n",
                "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
                "        optimizer.step()\n",
                "        scheduler.step()\n",
                "        \n",
                "        progress_bar.set_postfix({'loss': loss.item()})\n",
                "    \n",
                "    return total_loss / len(dataloader)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 11,
            "id": "eval_function",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Evaluation function\n",
                "def evaluate(model, dataloader, device):\n",
                "    model.eval()\n",
                "    predictions = []\n",
                "    true_labels = []\n",
                "    total_loss = 0\n",
                "    \n",
                "    with torch.no_grad():\n",
                "        for batch in tqdm(dataloader, desc=\"Evaluating\"):\n",
                "            input_ids = batch['input_ids'].to(device)\n",
                "            attention_mask = batch['attention_mask'].to(device)\n",
                "            labels = batch['labels'].to(device)\n",
                "            \n",
                "            outputs = model(\n",
                "                input_ids=input_ids,\n",
                "                attention_mask=attention_mask,\n",
                "                labels=labels\n",
                "            )\n",
                "            \n",
                "            total_loss += outputs.loss.item()\n",
                "            preds = torch.argmax(outputs.logits, dim=-1)\n",
                "            predictions.extend(preds.cpu().numpy())\n",
                "            true_labels.extend(labels.cpu().numpy())\n",
                "    \n",
                "    return predictions, true_labels, total_loss / len(dataloader)"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "training_loop",
            "metadata": {},
            "source": [
                "## 3. Train and Evaluate All Models"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 12,
            "id": "main_comparison",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "================================================================================\n",
                        "Training WangchanBERTa (airesearch/wangchanberta-base-att-spm-uncased)\n",
                        "================================================================================\n",
                        "\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Some weights of CamembertForSequenceClassification were not initialized from the model checkpoint at airesearch/wangchanberta-base-att-spm-uncased and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\n",
                        "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "\n",
                        "Epoch 1/3\n",
                        "------------------------------------------------------------\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "Training:   2%|‚ñè         | 67/3062 [00:44<33:30,  1.49it/s, loss=0.432]\n"
                    ]
                },
                {
                    "ename": "KeyboardInterrupt",
                    "evalue": "",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
                        "Cell \u001b[0;32mIn[12], line 51\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m60\u001b[39m)\n\u001b[0;32m---> 51\u001b[0m train_loss \u001b[38;5;241m=\u001b[39m train_epoch(model, train_loader, optimizer, scheduler, device)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrain_loss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     54\u001b[0m predictions, true_labels, val_loss \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, device)\n",
                        "Cell \u001b[0;32mIn[10], line 12\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, dataloader, optimizer, scheduler, device)\u001b[0m\n\u001b[1;32m      9\u001b[0m attention_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m---> 12\u001b[0m outputs \u001b[38;5;241m=\u001b[39m model(\n\u001b[1;32m     13\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m     14\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m     15\u001b[0m     labels\u001b[38;5;241m=\u001b[39mlabels\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     18\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     19\u001b[0m total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/camembert/modeling_camembert.py:1064\u001b[0m, in \u001b[0;36mCamembertForSequenceClassification.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1056\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1057\u001b[0m \u001b[38;5;124;03mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[1;32m   1058\u001b[0m \u001b[38;5;124;03m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[1;32m   1059\u001b[0m \u001b[38;5;124;03m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[1;32m   1060\u001b[0m \u001b[38;5;124;03m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[1;32m   1061\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   1062\u001b[0m return_dict \u001b[38;5;241m=\u001b[39m return_dict \u001b[38;5;28;01mif\u001b[39;00m return_dict \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39muse_return_dict\n\u001b[0;32m-> 1064\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mroberta(\n\u001b[1;32m   1065\u001b[0m     input_ids,\n\u001b[1;32m   1066\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mattention_mask,\n\u001b[1;32m   1067\u001b[0m     token_type_ids\u001b[38;5;241m=\u001b[39mtoken_type_ids,\n\u001b[1;32m   1068\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[1;32m   1069\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m   1070\u001b[0m     inputs_embeds\u001b[38;5;241m=\u001b[39minputs_embeds,\n\u001b[1;32m   1071\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m   1072\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m   1073\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m   1074\u001b[0m )\n\u001b[1;32m   1075\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1076\u001b[0m logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(sequence_output)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/camembert/modeling_camembert.py:889\u001b[0m, in \u001b[0;36mCamembertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    880\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[1;32m    882\u001b[0m embedding_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membeddings(\n\u001b[1;32m    883\u001b[0m     input_ids\u001b[38;5;241m=\u001b[39minput_ids,\n\u001b[1;32m    884\u001b[0m     position_ids\u001b[38;5;241m=\u001b[39mposition_ids,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    887\u001b[0m     past_key_values_length\u001b[38;5;241m=\u001b[39mpast_key_values_length,\n\u001b[1;32m    888\u001b[0m )\n\u001b[0;32m--> 889\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(\n\u001b[1;32m    890\u001b[0m     embedding_output,\n\u001b[1;32m    891\u001b[0m     attention_mask\u001b[38;5;241m=\u001b[39mextended_attention_mask,\n\u001b[1;32m    892\u001b[0m     head_mask\u001b[38;5;241m=\u001b[39mhead_mask,\n\u001b[1;32m    893\u001b[0m     encoder_hidden_states\u001b[38;5;241m=\u001b[39mencoder_hidden_states,\n\u001b[1;32m    894\u001b[0m     encoder_attention_mask\u001b[38;5;241m=\u001b[39mencoder_extended_attention_mask,\n\u001b[1;32m    895\u001b[0m     past_key_values\u001b[38;5;241m=\u001b[39mpast_key_values,\n\u001b[1;32m    896\u001b[0m     use_cache\u001b[38;5;241m=\u001b[39muse_cache,\n\u001b[1;32m    897\u001b[0m     output_attentions\u001b[38;5;241m=\u001b[39moutput_attentions,\n\u001b[1;32m    898\u001b[0m     output_hidden_states\u001b[38;5;241m=\u001b[39moutput_hidden_states,\n\u001b[1;32m    899\u001b[0m     return_dict\u001b[38;5;241m=\u001b[39mreturn_dict,\n\u001b[1;32m    900\u001b[0m )\n\u001b[1;32m    901\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    902\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/camembert/modeling_camembert.py:539\u001b[0m, in \u001b[0;36mCamembertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    528\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    529\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    530\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    536\u001b[0m         output_attentions,\n\u001b[1;32m    537\u001b[0m     )\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 539\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m layer_module(\n\u001b[1;32m    540\u001b[0m         hidden_states,\n\u001b[1;32m    541\u001b[0m         attention_mask,\n\u001b[1;32m    542\u001b[0m         layer_head_mask,\n\u001b[1;32m    543\u001b[0m         encoder_hidden_states,\n\u001b[1;32m    544\u001b[0m         encoder_attention_mask,\n\u001b[1;32m    545\u001b[0m         past_key_value,\n\u001b[1;32m    546\u001b[0m         output_attentions,\n\u001b[1;32m    547\u001b[0m     )\n\u001b[1;32m    549\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    550\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/camembert/modeling_camembert.py:470\u001b[0m, in \u001b[0;36mCamembertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    467\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    468\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 470\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m apply_chunking_to_forward(\n\u001b[1;32m    471\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeed_forward_chunk, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunk_size_feed_forward, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mseq_len_dim, attention_output\n\u001b[1;32m    472\u001b[0m )\n\u001b[1;32m    473\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    475\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pytorch_utils.py:238\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 238\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m forward_fn(\u001b[38;5;241m*\u001b[39minput_tensors)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/camembert/modeling_camembert.py:483\u001b[0m, in \u001b[0;36mCamembertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    481\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    482\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 483\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutput(intermediate_output, attention_output)\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/camembert/modeling_camembert.py:394\u001b[0m, in \u001b[0;36mCamembertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    393\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, hidden_states: torch\u001b[38;5;241m.\u001b[39mTensor, input_tensor: torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m torch\u001b[38;5;241m.\u001b[39mTensor:\n\u001b[0;32m--> 394\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    395\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[1;32m    396\u001b[0m     hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(hidden_states \u001b[38;5;241m+\u001b[39m input_tensor)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1736\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1734\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1735\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1736\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/module.py:1747\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1742\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1743\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1744\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1745\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1746\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1747\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m   1749\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1750\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
                        "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/torch/nn/modules/linear.py:125\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 125\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mlinear(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias)\n",
                        "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
                    ]
                }
            ],
            "source": [
                "# Main comparison loop\n",
                "results = []\n",
                "\n",
                "for model_name, model_id in MODELS.items():\n",
                "    print(f\"\\n{'='*80}\")\n",
                "    print(f\"Training {model_name} ({model_id})\")\n",
                "    print(f\"{'='*80}\\n\")\n",
                "    \n",
                "    # Load tokenizer and model\n",
                "    try:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(\n",
                "            model_id,\n",
                "            use_fast=False,\n",
                "            trust_remote_code=True\n",
                "        )\n",
                "    except:\n",
                "        tokenizer = AutoTokenizer.from_pretrained(\n",
                "            model_id,\n",
                "            trust_remote_code=True\n",
                "        )\n",
                "    \n",
                "    model = AutoModelForSequenceClassification.from_pretrained(\n",
                "        model_id,\n",
                "        num_labels=2,\n",
                "        trust_remote_code=True\n",
                "    )\n",
                "    model.to(device)\n",
                "    \n",
                "    # Create dataloaders\n",
                "    train_dataset = ToxicityDataset(X_train, y_train, tokenizer, MAX_LENGTH)\n",
                "    test_dataset = ToxicityDataset(X_test, y_test, tokenizer, MAX_LENGTH)\n",
                "    \n",
                "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
                "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
                "    \n",
                "    # Setup optimizer and scheduler\n",
                "    optimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\n",
                "    total_steps = len(train_loader) * EPOCHS\n",
                "    scheduler = get_linear_schedule_with_warmup(\n",
                "        optimizer,\n",
                "        num_warmup_steps=WARMUP_STEPS,\n",
                "        num_training_steps=total_steps\n",
                "    )\n",
                "    \n",
                "    # Training loop\n",
                "    best_f1 = 0\n",
                "    for epoch in range(EPOCHS):\n",
                "        print(f\"\\nEpoch {epoch + 1}/{EPOCHS}\")\n",
                "        print(\"-\" * 60)\n",
                "        \n",
                "        train_loss = train_epoch(model, train_loader, optimizer, scheduler, device)\n",
                "        print(f\"Training Loss: {train_loss:.4f}\")\n",
                "        \n",
                "        predictions, true_labels, val_loss = evaluate(model, test_loader, device)\n",
                "        \n",
                "        f1 = f1_score(true_labels, predictions, average='macro')\n",
                "        print(f\"Validation Loss: {val_loss:.4f}\")\n",
                "        print(f\"Macro F1: {f1:.4f}\")\n",
                "        \n",
                "        if f1 > best_f1:\n",
                "            best_f1 = f1\n",
                "            # Save best predictions\n",
                "            best_predictions = predictions\n",
                "            best_true_labels = true_labels\n",
                "    \n",
                "    # Calculate final metrics\n",
                "    accuracy = accuracy_score(best_true_labels, best_predictions)\n",
                "    precision = precision_score(best_true_labels, best_predictions, average='binary', pos_label=1)\n",
                "    recall = recall_score(best_true_labels, best_predictions, average='binary', pos_label=1)\n",
                "    f1 = f1_score(best_true_labels, best_predictions, average='binary', pos_label=1)\n",
                "    \n",
                "    results.append({\n",
                "        'Model': model_name,\n",
                "        'Accuracy': accuracy,\n",
                "        'Precision': precision,\n",
                "        'Recall': recall,\n",
                "        'F1-Score': f1\n",
                "    })\n",
                "    \n",
                "    print(f\"\\n{model_name} Final Results:\")\n",
                "    print(f\"  Accuracy:  {accuracy:.4f}\")\n",
                "    print(f\"  Precision: {precision:.4f}\")\n",
                "    print(f\"  Recall:    {recall:.4f}\")\n",
                "    print(f\"  F1-Score:  {f1:.4f}\")\n",
                "    \n",
                "    # Clear memory\n",
                "    del model, tokenizer, train_dataset, test_dataset, train_loader, test_loader\n",
                "    torch.cuda.empty_cache() if torch.cuda.is_available() else None"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "results",
            "metadata": {},
            "source": [
                "## 4. Results Comparison"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "results_table",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Create comparison table\n",
                "results_df = pd.DataFrame(results)\n",
                "\n",
                "print(\"\\n\" + \"=\"*80)\n",
                "print(\"FINAL COMPARISON RESULTS\")\n",
                "print(\"=\"*80 + \"\\n\")\n",
                "print(results_df.to_string(index=False))\n",
                "\n",
                "# Find best models\n",
                "best_acc_idx = results_df['Accuracy'].idxmax()\n",
                "best_f1_idx = results_df['F1-Score'].idxmax()\n",
                "\n",
                "print(f\"\\n{'='*80}\")\n",
                "print(f\"Best Model by Accuracy: {results_df.loc[best_acc_idx, 'Model']} ({results_df.loc[best_acc_idx, 'Accuracy']:.4f})\")\n",
                "print(f\"Best Model by F1-Score: {results_df.loc[best_f1_idx, 'Model']} ({results_df.loc[best_f1_idx, 'F1-Score']:.4f})\")\n",
                "print(\"=\"*80)"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "visualization",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Visualization\n",
                "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
                "fig.suptitle('Thai BERT Models Comparison', fontsize=16, fontweight='bold')\n",
                "\n",
                "metrics = ['Accuracy', 'Precision', 'Recall', 'F1-Score']\n",
                "colors = ['#3498db', '#e74c3c', '#2ecc71']\n",
                "\n",
                "for idx, metric in enumerate(metrics):\n",
                "    ax = axes[idx // 2, idx % 2]\n",
                "    bars = ax.bar(results_df['Model'], results_df[metric], color=colors)\n",
                "    ax.set_title(metric, fontsize=12, fontweight='bold')\n",
                "    ax.set_ylabel('Score', fontsize=10)\n",
                "    ax.set_ylim([0, 1])\n",
                "    ax.grid(axis='y', alpha=0.3)\n",
                "    \n",
                "    # Add value labels on bars\n",
                "    for bar in bars:\n",
                "        height = bar.get_height()\n",
                "        ax.text(bar.get_x() + bar.get_width()/2., height,\n",
                "                f'{height:.4f}',\n",
                "                ha='center', va='bottom', fontsize=9)\n",
                "\n",
                "plt.tight_layout()\n",
                "plt.savefig('thai_bert_comparison.png', dpi=150, bbox_inches='tight')\n",
                "plt.show()"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": null,
            "id": "save_results",
            "metadata": {},
            "outputs": [],
            "source": [
                "# Save results to CSV\n",
                "results_df.to_csv('thai_bert_comparison_results.csv', index=False)\n",
                "print(\"Results saved to 'thai_bert_comparison_results.csv'\")"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "base",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.12.4"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
